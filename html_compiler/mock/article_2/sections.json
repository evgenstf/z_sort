[{
  "type":"tldr",
  "content":"Вероятностное пространство. Случайная величина, ее распределение, матожидание, мода и медиана."
},
{"type":"markdown",
  "content":"**Примечание**: *Большинство приведённых ниже определений верны только для случая конечного множества элементарных исходов, хотя и сохраняют свой смысл при обобщении*\n \n# Вероятностное пространство\n \n*Вероятностное пространство* -- это тройка $(\\Omega, 2^{\\Omega}, P)$, где \n \n* $\\Omega$ -- множество <!элементарных исходов!>(Подробнее?){Исходы случайного эксперимента. Например, подбрасывания монеты. Обозначаются как $\\omega\\in\\Omega.$} \n* $2^{\\Omega}$ -- <!случайные события!>(А это что?){Подмножество множества случайных экспериментов $\\Omega$} \n* $P$ -- определенная на всех элементарных исходах функция вероятностÐ¸\n \n# Вероятность \n*Вероятность* -- функция $P(\\omega)$, которая каждому элементарному исходу сопоставляет число от 0 до 1.\n \n## Вероятность события \n*Вероятность события* -- сумма вероятностей составляющих его элементарных исходов: \n$$P(A) = \\sum_{\\omega\\in A}P(\\omega)$$\n \n* Если $P(A) = 0$, то событие $A$ *невозможное*. \n * Если $P(A) = 1$, то событие $A$ *достоверное*.\n \n## Независимость событий \nСобытия $A$ и $B$ называют *независимыми*, если вероятность наступления события $B$ не меняется при наступлении события $A$, и наоборот. То есть при $P(A) > 0, P(B) > 0$ выполнено: \n$$P(A\\cap B) = P(A)\\cdot P(B)$$\n \n* Если при $P(A) > 0, P(B) > 0$ выполнено $P(A\\cap B) = 0$, то события $A$ и $B$ называют *несовместными*.\n \nСобытия $A_1, A_2, . . . , A_n$ называются *попарно независимыми*, если любые два из них являются независимыми.\n \nСобытия $A_1, A_2, . . . , A_n$ называются *независимыми в совокупности*, если для любого подмножества этих событий $A_{i_1} , A{i_2} , . . . , A{i_k}$ выполнено: \n$$P(A_{i_1} \\cap A{i_2} \\cap . . . \\cap A{i_k}) = P(A_{i_1}) \\cdot P(A_{i_2}) \\cdot ... \\cdot P(A_{i_n})$$\n \n## Условная вероятность \n*Условная вероятность* -- вероятность наступления события $A$ при условии, что событие $B$ произошло, обозначается как $P(A|B)$. При $P(B) > 0$ выполнено: \n$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n\n \n## Полная группа событий \nСобытия $A_1, A_2, . . . , A_n$ называют *полной группой событий*, если любой элементарный исход принадлежит **ровно одному** из них. (То есть в результате проведения случайного эксперимента обязательно произойдет одно и только одно из событий $A_1, A_2, . . . , A_n$) \n* Если события $A_1, A_2, . . . , A_n$ образуют полную группу, то для любого события $B$ выполнено : \n$$P(B) = \\sum_{i =1}^n P(B|A_i)\\cdot P(A_i)$$\n \n# Случайная величина \n*Случайная величина* -- отображение $\\psi : \\Omega \\rightarrow R$ из множества исходов в множество вещественных значений.\n \n## Пример с кубиком \nПри броске игрального кубика множество элементарных исходов будут составлять выпадающие грани. Случайной величиной в таком случае может быть количество точек на выпавшей грани.\n \n![Playing dice](https://habrastorage.org/webt/vh/uk/lq/vhuklqliqxtkaki6kt0ybph4q-e.png) \nМожно заметить, что тогда для случайной величины можно посчитать ее вероятность, исходя из вероятностей элементарных исходов. В данном случае, например, вероятность получить при броске 4 равна $P(\\psi = 4) = \\frac{1}{6}$.\n \n## Распределение случайной величины\n \nЧаще всего при работе со случайной величиной хочется понимать ее поведение в совокупности на всем множестве значений. Для этого хорошо подходит ее *распределение*. Для дискретного случая, интересующего нас, это просто сама функция $P(\\psi = x)$, определяющая вероятность принятия случайной величиной значения $x$.\n \nНапример, вот так в общих чертах выглядит распределение роста мужчин и женщин: \n![](https://habrastorage.org/webt/3j/vo/3m/3jvo3m8ld-ldckzpif6olkkfqf8.png)\n \n## Математическое ожидание\n \n*Математическое ожидание* -- средневзвешанное значение случайной величины. Можно выразить двумя способами:\n \n* $E(\\psi) = \\sum_{\\omega \\in \\Omega} \\psi(\\omega) \\cdot P(\\omega)$ -- сумма произведений случайных значений всех возможных элементарных исходов на их вероятности \n* $E(\\psi) = \\sum_{x \\in V(\\psi)} x \\cdot P(\\psi = x)$ -- сумма произведений всех возможных <!случайных значений!>(В чем отличие от первого способа?){Мы объединили элементарные исходы по случайным значениям} на их вероятности\n \n## Мода и медиана\n \n*Мода случайной величины* -- значение, которое случайная величина примет с наибольшей вероятностью.\n \n*Медиана случайной величины* -- такое значение $x$, что $P(\\psi < x) \\leq \\frac{1}{2}$ и $P(\\psi > x) \\geq \\frac{1}{2}$. Другими словами,  половина исходов дает большее значение и половина меньшее.\n \n## Независимость случайной величины\n \nДве случайные величины $\\xi, \\psi$ нÐµзависимы, если  $\\forall x, y : P(\\xi = x, \\psi = y) = P(\\xi = x) \\cdot P(\\psi = y)$.\n \n## Свойства математического ожидания\n \n* *Линейность* -- математическое ожидание линейной комбинации двух случайных величин равно линейной комбинации их математических ожиданий: \n$$E(\\alpha \\psi + \\beta \\xi) = \\alpha E(\\psi) + \\beta E(\\xi)$$ **Не требует независимости случайных величин**\n \n* Если случайные величины независимы, то математическое ожидание их произведения равно произведению математических ожиданий: \n$$E(\\psi \\cdot \\xi)= E(\\psi) \\cdot E(\\xi)$$ **Требует независимости случайных величин**\n \n# Оценка времени работы\n \n* Для детерминированных алгоритмов под временем работы подразумевается время работы на худшем тесте.\n \n**TO BE CONTINUED**" }]
